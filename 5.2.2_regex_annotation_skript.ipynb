{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0fe2b2d",
   "metadata": {},
   "source": [
    "# 5.2.2\tComputergestützte Annotation mit manueller Kontrolle – das Annotationsskript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f0662",
   "metadata": {},
   "source": [
    "## 1. Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac00303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3c4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    \"\"\"\n",
    "    1. Asks for a csv file (and path if needed)\n",
    "    2. returns \"Success\" if file has been successfully read\n",
    "    3. returns an Error if the file was not found and restarts\n",
    "    4. if successfully read this function returns the csv file as a pandas dataframe\n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    #ask for the dataframe\n",
    "    while True:\n",
    "        try:\n",
    "            file = str(input(\"please enter the csv-file name and the path if needed\"))\n",
    "            df = pd.read_csv(file)\n",
    "            print(\"---------------- \\nSuccess\\n----------------\")\n",
    "            break\n",
    "        #Error handeling if file does not exists\n",
    "        except FileNotFoundError:\n",
    "                print(\"---------------- \\nError: file not found\\n----------------\")\n",
    "    return df\n",
    "\n",
    "def get_column(df):\n",
    "    \"\"\"\n",
    "    1. Needs a dataframe as input\n",
    "    2. Asks for the column name that should be processed\n",
    "    3. Checks if column name exists in dataframe\n",
    "    4. Returns \"Success\" if column name exists\n",
    "    5. Returns an Error if the column name doesn't exist and restarts\n",
    "    6. Returns the columnname as a string\n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    #ask for the columnname as an input\n",
    "    while True:\n",
    "        columnname = str(input(\"Please define the columnname where your text exists\"))\n",
    "        #check if columnname exists\n",
    "        if columnname in df.columns:\n",
    "            print(\"---------------- \\nColumnname successfully defined\\n----------------\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"---------------- \\nError: columnname not found\\n----------------\")\n",
    "    # return the columnname\n",
    "    return columnname\n",
    "\n",
    "def preprocessing_text(df, col):\n",
    "    \"\"\"\n",
    "    Uses a dataframe and preprocesses the text in the given column.\n",
    "    1. replace some chars\n",
    "    2. Creates two new column (job_desc, communication) and fills up the columns wit np.nan\n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    numpy as np\n",
    "    \"\"\"\n",
    "    #replace some chars\n",
    "    df[\"clean\"] = df[col].str.replace(\"-\\n\",\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"⸗\\n\",\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"#\",\" \")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"\\n\", \" \")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"'\",\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace('\"',\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\",,\",\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"„\",\"\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"ſ\", \"s\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"é\",\"e\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"è\",\"e\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"á\",\"a\")\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\"à\",\"a\")\n",
    "    \n",
    "    # learning from phase 2\n",
    "    df[\"clean\"] = df[\"clean\"].str.replace(\",\",\"\")\n",
    "    \n",
    "    df[\"job_desc\"] = np.nan\n",
    "    df[\"communication\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def return_list_as_dataframe(lst, define_columnname):\n",
    "    \"\"\"\n",
    "    Transfigures a list into a dataframe with one column and an index\n",
    "    1. Needs a list as input\n",
    "    2. Needs the columnname as a string to name the column of the dataframe\n",
    "    3. returns the dataframe\n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    df_lst = pd.DataFrame(data={define_columnname: lst})\n",
    "    return df_lst\n",
    "\n",
    "def get_frequency_of_Words_in_column(df_lst, columnname):\n",
    "    \"\"\"\n",
    "    Manipulates a dataframe with a text column to show the frequency of the values in one column\n",
    "    1. Needs a dataframe as input\n",
    "    2. Needs the columnname with text as input\n",
    "    3. Creates a new column \"Frequency\"\n",
    "    4. Groups the dataframe by the columnname and counts similar values\n",
    "    5. Manipulates dataframe where the defined columnname embodies a set of all values and \n",
    "    \"Frequency\" embodies the frequency of the values\n",
    "    6. Sort all rows by \"Frequency\"\n",
    "    7. Returns the manipulated dataframe\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    df_lst['Frequenz'] = df_lst.groupby(columnname)[columnname].transform('count')\n",
    "    df_lst = df_lst.drop_duplicates()\n",
    "    df_lst = df_lst.sort_values(by=['Frequenz'], ascending=False)\n",
    "    \n",
    "    return df_lst\n",
    "\n",
    "\n",
    "\n",
    "def process_rows(df):\n",
    "    \"\"\"\n",
    "    This function is combining several functions to process the rows of the given dataframe to annotate the text found in the cleaned text-column \"clean\" - created by the function preprocessing_text ()\n",
    "    1. needs a dataframe as an input\n",
    "    2. creates a list container for correction-words the user types in if the program does not do the right annotation\n",
    "    3. iterates through the rows of the dataframe column \"clean\" using the index\n",
    "    4. calls the function \"process_row\" to process every row and saves it in a variable \"output\"\n",
    "    5. checks if the user defines the suggested annotation is ok by calling annotation_is_ok()\n",
    "    6. While annotation_is_ok(output)== False the output stays the same\n",
    "    7. While annotation_is_ok(output)== True the user will be asked to copy the first words of the second part of the text in an input field\n",
    "        - This text is going to be saved in the list \"valid_tricky_words\"\n",
    "        - a new output will be created by using this text\n",
    "    8. New columns are created by the function add_new_cols()\n",
    "    9. Function asks if the user wants to end the program by calling function early_end()\n",
    "    10. If early_end()==True the function breaks and returns the dataframe and the vaid_tricky_words list\n",
    "    11. If early_end() == False the function continues with the next row\n",
    "    \n",
    "    Refers to the following functions in this script:\n",
    "    - process_row()\n",
    "    - annotation_is_not_ok()\n",
    "    - get_tricky_words()\n",
    "    - add_new_cols()\n",
    "    - early end()\n",
    "    \n",
    "    Used in:\n",
    "    - main()\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    # container of tricky words to save all tricky words during the loops and saves it by exit the script\n",
    "    valid_tricky_words = []\n",
    "    for i in df.index:\n",
    "        text = df[\"clean\"][i]\n",
    "        print('\\nProcessing row...\\n')\n",
    "        output = process_row(text, False)\n",
    "        while (annotation_is_not_ok(output)):\n",
    "            tricky_words = get_tricky_words()\n",
    "            valid_tricky_words.append(tricky_words)\n",
    "            output = process_row(text, tricky_words)\n",
    "        df = add_new_cols(df, i, output)\n",
    "        if (early_end()):\n",
    "            break;\n",
    "    return {'df': df, 'valid_tricky_words': valid_tricky_words}\n",
    "\n",
    "def process_row(text, tricky_words):\n",
    "    \"\"\"\n",
    "    1. needs text input and tricky_words from process_rows()\n",
    "    2. uses the variable text from process_rows()\n",
    "    3. calls get_regex() and saves it in a variable if tricky_words == True it used the word_sequence from the user\n",
    "    4. returns a dict with the two key \"job_desc\" and \"communication\"\n",
    "    5. creates two strings as values by calling the functions get_job_desc() and get_communication()\n",
    "    \n",
    "    Refers to the following functions in this script:\n",
    "    - get_regex()\n",
    "    - get_job_desc()\n",
    "    - get_communication()\n",
    "    \n",
    "    Used in: \n",
    "    - process_rows()\n",
    "    \"\"\"\n",
    "    regex = get_regex(tricky_words)\n",
    "    return {\n",
    "        'text': text,\n",
    "        'job_desc': get_job_desc(regex, text),\n",
    "        'communication': get_communication(regex, text)\n",
    "    }\n",
    "\n",
    "def get_regex(tricky_words):\n",
    "    \"\"\"\n",
    "    1. provides the default regexes for the job ad and the communication\n",
    "    2. defines a regex_default by creating two captured groups (job ad, communication) with format()\n",
    "        - re_text is always applied and captured in the group \"job_desc\"\n",
    "        - re_address, re_street, re_id_number are arranged hierarchically\n",
    "    3. if tricky_words == True it defines a regex_tricky_words with captured groups (job, communication)\n",
    "    4. returns regex_default if tricky_words == False, else returns re_tricky_words\n",
    "    \n",
    "    Used in: \n",
    "    - process_row()\n",
    "    \"\"\"\n",
    "    #collect all the starting text in a non-greedy way\n",
    "    re_text = '(.)*?'\n",
    "    # address has different forms\n",
    "    # No arabic numbers; no comma\n",
    "    re_address = '\\s[VIX]+[.]?\\s.+$'\n",
    "    re_street = '(\\s\\w+)\\s?\\W?([gG]asse|[sS]tra(ss|ß)e|[wW]eg|[pP]latz|[pP]fad|[uU]fer|[aA]llee|[aA]nlage|[gG]raben|[cC]haussee|[wW]ache|[pP]romenade|[pP]forte|[dD]amm|[tT]or|[bB]rücke|[rR]ing|[lL]ände)\\s(.+$)'\n",
    "    #([.]?\\d+) excluded\n",
    "    re_id_number = '\\s?\\d+\\s?[-]?\\d+\\w?\\W?\\s*$'\n",
    "    # Hyphen and some more variations (1non-word character, 1 letter) allowed or several spaces at the end \n",
    "    \n",
    "    #final regex\n",
    "    #default case\n",
    "    regex_default = \"^(?P<job_desc>{})(?P<communication>{}|{}|{})\".format(re_text, re_address, re_street, re_id_number).format(re_text,re_address,re_street) \n",
    "    #tricky words case\n",
    "    re_tricky_words = \"{}.*\".format(tricky_words)\n",
    "    regex_tricky_words = \"^(?P<job_desc>{})(?P<communication>{})\".format(re_text,re_tricky_words) \n",
    "    return regex_tricky_words if tricky_words else regex_default\n",
    "\n",
    "\n",
    "def apply_regex(regex, text):\n",
    "    \"\"\"\n",
    "    Function to apply the regex (variable with string) by using the function search() from the re-module\n",
    "    1. needs a regex in form of a string\n",
    "    2. needs a text in form of a string\n",
    "    3. returns the text grouped by regex (groups: job_desc; communication)case sensitivity is ignored\n",
    "    \n",
    "    Used in:\n",
    "    - get_job_desc\n",
    "    - get_communication\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    re\n",
    "    \"\"\"\n",
    "    return re.search(regex, text, re.IGNORECASE)\n",
    "\n",
    "def get_job_desc(regex, text):\n",
    "    \"\"\"\n",
    "    Uses the function group() from the re-module to get the subgroup \"job_desc\" defined in get_regex()\n",
    "    1. needs a regex in form of a string\n",
    "    2. needs a text in form of a string\n",
    "    3. returns the sub-group \"job_desc\" as a string\n",
    "    Refers to:\n",
    "    - apply_regex()\n",
    "    Used in:\n",
    "    - process_row()\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    re\n",
    "    \"\"\"\n",
    "    if apply_regex(regex, text) is None:\n",
    "        return None\n",
    "    \n",
    "    return apply_regex(regex, text).group('job_desc')\n",
    "\n",
    "def get_communication(regex, text):\n",
    "    \"\"\"\n",
    "    Uses the function group() from the re-module to get the subgroup \"communication\" defined in get_regex()\n",
    "    1. needs a regex in form of a string\n",
    "    2. needs a text in form of a string\n",
    "    3. returns the sub-group \"communication\" as a string\n",
    "    Used in:\n",
    "    - process_row()\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    re\n",
    "    \"\"\"\n",
    "    if apply_regex(regex, text) is None:\n",
    "        return None\n",
    "    \n",
    "    return apply_regex(regex, text).group('communication')\n",
    "\n",
    "def annotation_is_not_ok(output):\n",
    "    \"\"\"\n",
    "    1. Needs the output defined in process_rows()\n",
    "    2. Prints out the suggested annotations job_desc and communication\n",
    "    3. asks if the annotation is OK\n",
    "    4. returns answer (True|False) \n",
    "    \n",
    "    Used in:\n",
    "    - process_rows()\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    print(('\\n\\nGesamte Stellenanzeige: \\n >{}\\n\\n'.format(output['text'])))\n",
    "    print('Stellenbeschreibung: \\n >{}\\n\\n'.format(output['job_desc']))\n",
    "    print('Kommunikationsbeschreibung: \\n >{}\\n\\n'.format(output['communication']))\n",
    "    print('')\n",
    "    # asks if the separation is OK. True is not OK, False is OK\n",
    "    while True:\n",
    "        answer = str(input(\"\\nIs there a mistake in the annotation? (Y/N)\\n\"))\n",
    "        if answer == \"Y\": \n",
    "            return True\n",
    "        elif answer == \"N\": \n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "def get_tricky_words():\n",
    "    \"\"\"\n",
    "    1. Asks for text input\n",
    "    2. Returns text input as astring \n",
    "    \n",
    "    Used in:\n",
    "    - process_rows()\n",
    "    \"\"\"\n",
    "    # ask for the new words and returns them as string\n",
    "    return str(input(\"\\nPlease copy the first two words of the second text part and save it here.\\nIf the text cannot be annotated in two parts, please write a dollar sign '$' in the field. \\n\"))\n",
    "\n",
    "def add_new_cols(df, idx, output):\n",
    "    \"\"\"\n",
    "    1. Needs the dataframe\n",
    "    2. Needs the index from process_rows()\n",
    "    3. Needs the output from process_rows()\n",
    "    4. Commands the column \"job_desc\" with the defined index to save the value from the key \"job_desc\", defined in process_row\n",
    "    5. Commands the column \"communication\" with the defined index to save the value from the key \"communication\", defined in process_row\n",
    "    6. Returns the dataframe\n",
    "    \n",
    "    Used in:\n",
    "    - process_rows()\n",
    "    \n",
    "    The following libraries are needed:\n",
    "    pandas as pd\n",
    "    \"\"\"\n",
    "    df.loc[df.index[idx], \"job_desc\"] = output[\"job_desc\"]\n",
    "    df.loc[df.index[idx], \"communication\"] = output[\"communication\"]\n",
    "    return df\n",
    "\n",
    "def early_end():\n",
    "    \"\"\"\n",
    "    1. Asks for an (Y/N) input\n",
    "    2. Returns True if input == \"Y\" and False if input == \"N\"\n",
    "    Used in:\n",
    "    - process_rows()\n",
    "    \"\"\"\n",
    "    \n",
    "    # ask if the user want to finish now\n",
    "    # True to end now, False to continue with\n",
    "    # the next row\n",
    "    while True:\n",
    "        decision = str(input(\"\\nDo you want to stop the program? (Y/N)\\n\"))\n",
    "        if decision == \"Y\":\n",
    "            print(\"\\n\\nThank you for using this program. Be aware that the annotated korpus is now available in the csv-file 'output.csv' in the same folder as this script.\\nYou will also find a csv file with all copied word sequences and their frequencies you put in correcting this program. \\nIf you want to use this program again please save it in a new folder to avoid overwrite the current output.\\n\\n\")\n",
    "            return True\n",
    "        elif decision == \"N\":\n",
    "            return False\n",
    "        else:\n",
    "            print(\"\\n*** Wrong Command ***\\n\") \n",
    "            \n",
    "# run main program\n",
    "def main():\n",
    "    df = get_dataframe()\n",
    "    col = get_column(df)\n",
    "    #text preprocessing\n",
    "    df = preprocessing_text(df, col)\n",
    "    #process each row of the dataframe and save the annotated groups in several columns\n",
    "    output = process_rows(df)\n",
    "    #export output as csv\n",
    "    output['df'].to_csv('output.csv', index=False)\n",
    "    #save valid_tricky_words in dataframe\n",
    "    valid_tricky_words = output['valid_tricky_words']\n",
    "    df_tricky_words = return_list_as_dataframe(valid_tricky_words, \"Wörter\")\n",
    "    #manipulate dataframe to show the frequencys of the value sets\n",
    "    df_tricky_words = get_frequency_of_Words_in_column(df_tricky_words, \"Wörter\")\n",
    "    #export as csv\n",
    "    df_tricky_words.to_csv('num_of_tricky_words.csv', index=False)\n",
    "    \n",
    "    print('correction words: ', output['valid_tricky_words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031840ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run main program\n",
    "main()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be471e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
